{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import tensorflow as tf # tensorflow \n","from tensorflow import keras # Keras\n","from keras import layers # Keras.Layers\n","\n","from sklearn.metrics import r2_score\n","\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","#for dirname, _, filenames in os.walk('/kaggle/input'):\n","#    for filename in filenames:\n","#        print(os.path.join(dirname, filename))\n","        \n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T08:20:20.454422Z","iopub.execute_input":"2021-09-27T08:20:20.454880Z","iopub.status.idle":"2021-09-27T08:20:26.690282Z","shell.execute_reply.started":"2021-09-27T08:20:20.454792Z","shell.execute_reply":"2021-09-27T08:20:26.689387Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Variables & functions\n","\n","book_input_dir='/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/'\n","book_input_val_dir='/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/'\n","book_input_test_dir='/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/'\n","trade_input_dir='/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet/'\n","trade_input_test_dir='/kaggle/input/optiver-realized-volatility-prediction/trade_test.parquet/'\n","target_train_data=pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n","target_test= '/kaggle/input/optiver-realized-volatility-prediction/test.csv'\n","#time_split=[0,60,120,180,240,300,360,420,480,540,600]\n","time_split=[0,100,200,300,400,500,600]\n","TIME_INTERVAL=len(time_split)-1\n","NUM_FEATURES_RNN=6\n","key_fields=['time_id','stock_id']\n","val_files_list = []\n","train_files_list = []\n","test_files_list = []\n","\n","\n","def log_return(list_stock_prices):\n","    return np.log(list_stock_prices).diff() \n","\n","def realized_volatility(series_log_return):\n","    return np.sqrt(np.sum(series_log_return**2))\n","\n","def get_wap1(df):\n","    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n","    b = df['bid_size1'] + df['ask_size1']\n","    return a1/b\n","\n","def get_wap2(df):\n","    a1 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n","    b = df['bid_size2'] + df['ask_size2']\n","    return a1/b\n","\n","def get_wap(df):\n","    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n","    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n","    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']\n","    return (a1 + a2)/ b\n","\n","\n","def get_wap3(df):\n","    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n","    return wap\n","\n","def get_wap4(df):\n","    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n","    return wap\n","\n","def get_trade_wap(df):\n","    return df['price'] * df['size'] /  df['size']\n","\n","def get_trade_wap_count(df):\n","    return (df['price'] * df['size'] * df['order_count']) / ((df['size'] * df['order_count']))\n","\n","\n","def normalize_trade(df):\n","    result = df.copy()\n","    for feature_name in ['size', 'order_count']:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","    return result\n","\n","def normalize_trade_osum(df):\n","    result = df.copy()\n","    for feature_name in ['osum']:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","    return result\n","\n","def normalize_book(df):\n","    result = df.copy()\n","    for feature_name in ['log_return', 'wap']:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","    return result\n","\n","def normalize_book_data(df):\n","    result = df.copy()\n","    for feature_name in ['price_spread', 'price_spread2', 'bid_spread', 'ask_spread', 'bid_ask_spread']:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","    return result\n","\n","book_files = tf.data.Dataset.list_files(book_input_dir+\"*\")\n","for book_file in book_files:\n","    stock_id = tf.strings.split(book_file,\"=\")[-1]\n","    stock_id = tf.strings.to_number(stock_id, tf.int32).numpy()\n","    val_file = (8,19,27,36,45,54,63,72,82,99,110,121)\n","    \n","    if stock_id in val_file:\n","        val_files_list.append(book_file)\n","    #elif stock_id not in val_file:\n","    elif stock_id < 10:\n","        train_files_list.append(book_file)\n","        \n","test_files = tf.data.Dataset.list_files(book_input_test_dir+\"*\")\n","for test_file in test_files:\n","    test_files_list.append(test_file)"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:35:51.028544Z","iopub.execute_input":"2021-09-27T09:35:51.029079Z","iopub.status.idle":"2021-09-27T09:35:51.473763Z","shell.execute_reply.started":"2021-09-27T09:35:51.029047Z","shell.execute_reply":"2021-09-27T09:35:51.472892Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":["***How Data defined for RNN?***\n","TBC "],"metadata":{}},{"cell_type":"code","source":["# Preprocess function\n","# reads book and trade parquet files as dataframes\n","# - adds stock ID to dataframe\n","# - derives wap and log_return for book data\n","# - aggregates wap/log_return as per time bin\n","# - normalizes trade data and aggregates price/size/order_count as per time bin\n","# - merges book, trade with target value (key - time_id and stock_id)\n","# returns data (as time-series ragged tensor) and target values.\n","\n","def preprocess_rnn_data(mode, book_file, trade_file, stock_id):\n","    \n","    print(book_file, trade_file, type(stock_id))\n","    \n","    # Get All data in required format and combine with target\n","    book = pd.read_parquet(book_file)\n","    #trade = pd.read_parquet(trade_file)\n","    \n","    book['stock_id'] = stock_id\n","    #trade['stock_id'] = stock_id\n","    \n","    #Process book data\n","    book['id'] = str(stock_id) + '-' + book['time_id'].astype(str) \n","    book['wap1'] = get_wap1(book) \n","    book.loc[:,'log_return1'] = log_return(book['wap1'])\n","    book = book[~book['log_return1'].isnull()]\n","    \n","    book['wap2'] = get_wap2(book) \n","    book.loc[:,'log_return2'] = log_return(book['wap2'])\n","    book = book[~book['log_return2'].isnull()]\n","    \n","    book['wap3'] = get_wap3(book) \n","    book.loc[:,'log_return3'] = log_return(book['wap3'])\n","    book = book[~book['log_return3'].isnull()]\n","    \n","    book['wap4'] = get_wap4(book) \n","    book.loc[:,'log_return4'] = log_return(book['wap4'])\n","    book = book[~book['log_return4'].isnull()]\n","    \n","    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n","    book['price_spread2'] = (book['ask_price2'] - book['bid_price2']) / ((book['ask_price2'] + book['bid_price2']) / 2)\n","    \n","    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n","    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n","    book[\"bid_ask_spread\"] = abs(book['bid_spread'] - book['ask_spread'])\n","    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n","    # drop columns\n","    #book = book.drop(columns=['bid_price1','ask_price1','bid_price2','ask_price2','bid_size1','ask_size1','bid_size2','ask_size2'])\n","    \n","    # aggregate by time bins\n","    #book = normalize_book(book)\n","    bins = pd.cut(book['seconds_in_bucket'], time_split) # split for every minute\n","    book_data = book.groupby(['time_id','stock_id',bins]).aggregate(\n","        vol1=('log_return1', realized_volatility),\n","        vol3=('log_return3', realized_volatility),\n","        vol4=('log_return4', realized_volatility),\n","        vol2=('log_return2', realized_volatility),\n","        #wap1=('wap1', np.std),\n","        #wap2=('wap2', np.std),\n","        #wap3=('wap3', np.std),\n","        #wap4=('wap4', np.std),\n","        ps1=('price_spread', np.std),\n","        ps2=('price_spread2', np.std),\n","        #bs_std=('bid_spread', np.std),\n","        #as_std=('ask_spread', np.std),\n","        bas_std=('bid_ask_spread', np.std)\n","    ).fillna(0)\n","   \n","    book_data = book_data.groupby(key_fields).agg(tuple).applymap(list).reset_index()\n","\n","    if mode == \"train\":\n","        data = pd.merge(book_data,target_train_data,on=key_fields)\n","    else:\n","        data = book_data\n","    \n","    \n","    # Get data into 5x5 matrix\n","    data['final'] = data.apply(lambda d: np.column_stack((d['vol1'], d['vol2'], d['bas_std'], d['ps1'], d['ps2'], d['bas_std'])), axis=1)\n","    #size = data['target'].size\n","    size = len(data)\n","    rnn_data = tf.ragged.constant(\n","        [data['final']],\n","        inner_shape=(size, TIME_INTERVAL, NUM_FEATURES_RNN))\n","    sid = tf.fill(size, stock_id)\n","    \n","    if mode == \"train\":\n","        return {'data': rnn_data, 'stockID': sid}, data['target']\n","    else:\n","        return {'data': rnn_data, 'stockID': sid}"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:34:54.976062Z","iopub.execute_input":"2021-09-27T09:34:54.976470Z","iopub.status.idle":"2021-09-27T09:34:54.997421Z","shell.execute_reply.started":"2021-09-27T09:34:54.976431Z","shell.execute_reply":"2021-09-27T09:34:54.996447Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["data = preprocess_rnn_data(\"train\", '/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=66',\n","                '/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=66',\n","               66)"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:33:32.457427Z","iopub.execute_input":"2021-09-27T09:33:32.457809Z","iopub.status.idle":"2021-09-27T09:34:03.216271Z","shell.execute_reply.started":"2021-09-27T09:33:32.457771Z","shell.execute_reply":"2021-09-27T09:34:03.215442Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# RNN Generator\n","def gen_rnn_train_dataset():\n","       for book_file in train_files_list:    \n","            stock_id = tf.strings.split(book_file,\"/\")[-1]\n","            stock_id_num = tf.strings.split(stock_id, '=')[-1]\n","            trade_file = trade_input_dir+stock_id\n","            processed_data = preprocess_rnn_data(\"train\", book_file.numpy().decode('ascii') ,\n","                                             trade_file.numpy().decode('ascii'),\n","                                             tf.strings.to_number(stock_id_num, tf.int32).numpy())\n","                \n","            \n","            yield processed_data # tuple of ragged time-series tensor and target\n","            \n","def gen_rnn_val_dataset():\n","       for book_file in val_files_list:    \n","            stock_id = tf.strings.split(book_file,\"/\")[-1]\n","            stock_id_num = tf.strings.split(stock_id, '=')[-1]\n","            trade_file = trade_input_dir+stock_id\n","            processed_data = preprocess_rnn_data(\"train\", book_file.numpy().decode('ascii') ,\n","                                             trade_file.numpy().decode('ascii'),\n","                                             tf.strings.to_number(stock_id_num, tf.int32).numpy())\n","                \n","           \n","            yield processed_data # tuple of ragged time-series tensor and target\n","            \n","#tensorflow Dataset \n","# cache data based on files\n","# preprocess by creating realized vol for every n seconds\n","# combine with trade data every n seconds\n","# returns dataset having list of avg(price), avg(order) and realized vol per n secondshelp\n","\n","train_rnn_data = tf.data.Dataset.from_generator(gen_rnn_train_dataset, \n","                                               output_signature= (\n","                                                {'data':tf.TensorSpec(shape=(None,TIME_INTERVAL, NUM_FEATURES_RNN), dtype=tf.float64, name='data'),\n","                                                 'stockID':tf.TensorSpec(shape=(None,), dtype=tf.int16, name='stockID')},\n","                                                tf.TensorSpec(shape=(None,), dtype=tf.float64, name='target'),\n","                                           ))\n","train_rnn_data=train_rnn_data.prefetch(buffer_size=5)\n","train_rnn_data=train_rnn_data.unbatch()\n","train_rnn_data=train_rnn_data.batch(32)\n","train_rnn_data=train_rnn_data.cache()\n","# Shuffle,repeat,batch ??\n","\n","#val dataset\n","val_rnn_data = tf.data.Dataset.from_generator(gen_rnn_val_dataset, \n","                                               output_signature= (\n","                                                {'data':tf.TensorSpec(shape=(None,TIME_INTERVAL, NUM_FEATURES_RNN), dtype=tf.float64, name='data'),\n","                                                 'stockID':tf.TensorSpec(shape=(None,), dtype=tf.int16, name='stockID')},\n","                                                tf.TensorSpec(shape=(None,), dtype=tf.float64, name='target'),\n","                                           ))\n","val_rnn_data=val_rnn_data.prefetch(buffer_size=5)\n","val_rnn_data=val_rnn_data.unbatch()\n","val_rnn_data=val_rnn_data.batch(32)\n","val_rnn_data=val_rnn_data.cache()"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:35:35.713380Z","iopub.execute_input":"2021-09-27T09:35:35.713784Z","iopub.status.idle":"2021-09-27T09:35:35.779930Z","shell.execute_reply.started":"2021-09-27T09:35:35.713744Z","shell.execute_reply":"2021-09-27T09:35:35.779050Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["def rmspe(y_true, y_pred):\n","    return  (tf.sqrt(tf.reduce_mean(tf.square((y_true - y_pred) / y_true))))\n","\n","def r2score(y, y_pred):\n","  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n","  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n","  r2 = tf.subtract(1.0, tf.divide(residual, total))\n","  return r2"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T09:35:36.203463Z","iopub.execute_input":"2021-09-27T09:35:36.204104Z","iopub.status.idle":"2021-09-27T09:35:36.210109Z","shell.execute_reply.started":"2021-09-27T09:35:36.204067Z","shell.execute_reply":"2021-09-27T09:35:36.209197Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["#Build Model\n","CELLS=256\n","EMBEDDING_SIZE=8\n","OUTPUT_RNN=32\n","OUTPUT_FRNN=8\n","\n","input_rnn = keras.Input(shape=(TIME_INTERVAL, NUM_FEATURES_RNN), name='data')\n","sID = keras.Input(shape=(1), name='stockID')\n","flat_rnn = layers.Flatten()(input_rnn)\n","\n","\n","#RNN Processing\n","rnn = layers.GRU(CELLS, activation='relu', return_sequences=True)(input_rnn)\n","rnn = layers.GRU(OUTPUT_RNN, activation='relu')(rnn)\n","\n","#Flat RNN Processing \n","frnn = keras.layers.Dense(CELLS, activation='relu')(flat_rnn)\n","frnn = keras.layers.Dense(128, activation='relu')(flat_rnn)\n","frnn = layers.Dense(OUTPUT_FRNN, activation='relu')(frnn)\n","\n","#StockID Embedding\n","sembed = layers.Embedding(127, EMBEDDING_SIZE, input_length=1)(sID)\n","embed = layers.Flatten()(sembed)\n","\n","#Concatenated Processing\n","combined = layers.concatenate([rnn, frnn, embed])\n","combined = layers.Dense(CELLS,activation='relu')(combined)\n","#combined = keras.layers.BatchNormalization()(combined)\n","final = layers.Dense(CELLS, activation='relu')(combined)\n","final = layers.Dense(CELLS, activation='relu')(combined)\n","final = layers.Dropout(.3)(final)\n","final = layers.Dense(128, activation='relu')(final)\n","final = layers.Dense(32, activation='relu')(final)\n","final = layers.Dense(1)(final)\n","\n","model= keras.models.Model(inputs=[input_rnn, sID], outputs=final)\n","        \n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=6)\n","model.compile(optimizer=keras.optimizers.Adam(),\n","              loss=rmspe, \n","              metrics=[r2score, keras.metrics.MeanSquaredError()])\n","    \n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:54:02.047138Z","iopub.execute_input":"2021-09-27T10:54:02.047537Z","iopub.status.idle":"2021-09-27T10:54:02.369664Z","shell.execute_reply.started":"2021-09-27T10:54:02.047497Z","shell.execute_reply":"2021-09-27T10:54:02.368501Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":["keras.utils.plot_model(model)"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:27:32.493404Z","iopub.execute_input":"2021-09-27T10:27:32.493754Z","iopub.status.idle":"2021-09-27T10:27:32.666016Z","shell.execute_reply.started":"2021-09-27T10:27:32.493726Z","shell.execute_reply":"2021-09-27T10:27:32.664884Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["history = model.fit(x=train_rnn_data, validation_data=val_rnn_data,\n","                    epochs=30, verbose=1, callbacks=[callback])"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:03.908388Z","iopub.execute_input":"2021-09-27T11:08:03.908783Z","iopub.status.idle":"2021-09-27T11:16:02.522115Z","shell.execute_reply.started":"2021-09-27T11:08:03.908750Z","shell.execute_reply":"2021-09-27T11:16:02.521350Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(history.history)[['loss','val_loss']].plot()"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:17:11.497640Z","iopub.execute_input":"2021-09-27T11:17:11.498029Z","iopub.status.idle":"2021-09-27T11:17:11.686620Z","shell.execute_reply.started":"2021-09-27T11:17:11.497998Z","shell.execute_reply":"2021-09-27T11:17:11.685509Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(history.history)[['r2score','val_r2score']].plot()"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:17:15.261648Z","iopub.execute_input":"2021-09-27T11:17:15.262040Z","iopub.status.idle":"2021-09-27T11:17:15.450568Z","shell.execute_reply.started":"2021-09-27T11:17:15.262006Z","shell.execute_reply":"2021-09-27T11:17:15.449794Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":["def gen_test_dataset():\n","       processed_data = []\n","       for book_file in test_files_list:    \n","            stock_id = tf.strings.split(book_file,\"/\")[-1]\n","            stock_id_num = tf.strings.split(stock_id, '=')[-1]\n","            trade_file = trade_input_dir+stock_id\n","            data = preprocess_rnn_data(\"test\",book_file.numpy().decode('ascii') ,\n","                                             trade_file.numpy().decode('ascii'),\n","                                             tf.strings.to_number(stock_id_num, tf.int32).numpy())\n","                \n","            processed_data = processed_data.append(data)\n","            return processed_data # tuple of ragged time-series tensor and target"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:48:35.775175Z","iopub.execute_input":"2021-09-27T11:48:35.775588Z","iopub.status.idle":"2021-09-27T11:48:35.782834Z","shell.execute_reply.started":"2021-09-27T11:48:35.775553Z","shell.execute_reply":"2021-09-27T11:48:35.781731Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":["#Get Test Data\n","test_data = tf.data.Dataset.from_generator(gen_test_dataset, \n","                                           output_signature= (\n","                                             {'data':tf.TensorSpec(shape=(None,TIME_INTERVAL, NUM_FEATURES_RNN), dtype=tf.float64, name='data'),\n","                                                 'stockID':tf.TensorSpec(shape=(None,), dtype=tf.int16, name='stockID')}\n","                                           ))\n","#test_data=test_data.prefetch(buffer_size=5)\n","#test_data=test_data.unbatch()\n","#test_data=test_data.batch(1)"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:21:54.860218Z","iopub.execute_input":"2021-09-27T11:21:54.860960Z","iopub.status.idle":"2021-09-27T11:21:54.887701Z","shell.execute_reply.started":"2021-09-27T11:21:54.860921Z","shell.execute_reply":"2021-09-27T11:21:54.886665Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":["test_data = gen_test_dataset()"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:52:31.935790Z","iopub.execute_input":"2021-09-27T11:52:31.936150Z","iopub.status.idle":"2021-09-27T11:52:32.038210Z","shell.execute_reply.started":"2021-09-27T11:52:31.936122Z","shell.execute_reply":"2021-09-27T11:52:32.037399Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":["tdata = preprocess_rnn_data(\"test\",'/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=10',\n","                '/kaggle/input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=10',\n","               10)"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:34:55.466308Z","iopub.execute_input":"2021-09-27T11:34:55.466747Z","iopub.status.idle":"2021-09-27T11:35:30.504852Z","shell.execute_reply.started":"2021-09-27T11:34:55.466714Z","shell.execute_reply":"2021-09-27T11:35:30.503693Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":["result = model.predict_generator(test_data)"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:52:20.833235Z","iopub.execute_input":"2021-09-27T11:52:20.833625Z","iopub.status.idle":"2021-09-27T11:52:20.874496Z","shell.execute_reply.started":"2021-09-27T11:52:20.833591Z","shell.execute_reply":"2021-09-27T11:52:20.872597Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["# Model evaluate with test data and write output\n","result = model.predict(tf.convert_to_tensor([[2.941987e-04, 2.506079e-04, 1.000000e+00, 1.00000,1.000000e+00,1.000000e+00]]))"],"metadata":{"execution":{"iopub.status.busy":"2021-09-27T10:41:28.312064Z","iopub.execute_input":"2021-09-27T10:41:28.312447Z","iopub.status.idle":"2021-09-27T10:41:28.397228Z","shell.execute_reply.started":"2021-09-27T10:41:28.312417Z","shell.execute_reply":"2021-09-27T10:41:28.395173Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]}]}